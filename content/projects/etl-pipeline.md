---
title: "ETL Pipeline with Python & Airflow"
date: 2025-01-15
draft: false
description: "A complete ETL pipeline to extract, transform, and load data from an API to PostgreSQL"
tags: ["python", "etl", "airflow", "postgresql", "data-engineering"]
categories: ["projects"]
featuredImage: "/images/projects/etl-pipeline.png"
---

## ðŸ“‹ Project Overview

This project demonstrates building a complete **ETL pipeline** (Extract, Transform, Load) using Python and Apache Airflow to orchestrate the data flow.

<div class="intro-block">

**Goal**: Extract weather data from a public API, transform it into actionable metrics, and load it into a PostgreSQL database for analysis.

</div>

## ðŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-------------|
| Orchestration | Apache Airflow |
| Language | Python 3.11 |
| Database | PostgreSQL 15 |
| Containerization | Docker Compose |
| Testing | pytest |

## ðŸ“ Project Structure

```
etl-weather-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ weather_etl_dag.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ðŸ’» Main Code

### Airflow DAG

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'benjamin',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'weather_etl_pipeline',
    default_args=default_args,
    description='ETL pipeline for weather data',
    schedule_interval='@daily',
    catchup=False,
) as dag:

    extract_task = PythonOperator(
        task_id='extract_weather_data',
        python_callable=extract_weather_data,
    )

    transform_task = PythonOperator(
        task_id='transform_weather_data',
        python_callable=transform_weather_data,
    )

    load_task = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
    )

    extract_task >> transform_task >> load_task
```

## ðŸ“Š Results

- **Volume Processed**: ~10,000 records/day
- **Execution Time**: < 2 minutes
- **Reliability**: 99.5% uptime over 30 days

## ðŸ”— Links

- [**Source Code on GitHub**](https://github.com/ton-username/etl-weather-pipeline)
- [Airflow Documentation](https://airflow.apache.org/docs/)

---

*Project completed as part of my Data Engineering training.*
