---
title: "Pipeline ETL avec Python & Airflow"
date: 2025-01-15
draft: false
description: "Un pipeline ETL complet pour extraire, transformer et charger des donnÃ©es depuis une API vers PostgreSQL"
tags: ["python", "etl", "airflow", "postgresql", "data-engineering"]
categories: ["projects"]
featuredImage: "/images/projects/etl-pipeline.png"
---

## ğŸ“‹ AperÃ§u du projet

Ce projet dÃ©montre la crÃ©ation d'un **pipeline ETL** (Extract, Transform, Load) complet utilisant Python et Apache Airflow pour orchestrer le flux de donnÃ©es.

<div class="intro-block">

**Objectif** : Extraire des donnÃ©es mÃ©tÃ©o depuis une API publique, les transformer en mÃ©triques exploitables, et les charger dans une base PostgreSQL pour analyse.

</div>

## ğŸ› ï¸ Stack technique

| Composant | Technologie |
|-----------|-------------|
| Orchestration | Apache Airflow |
| Langage | Python 3.11 |
| Base de donnÃ©es | PostgreSQL 15 |
| Conteneurisation | Docker Compose |
| Tests | pytest |

## ğŸ“ Structure du projet

```
etl-weather-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ weather_etl_dag.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ’» Code principal

### DAG Airflow

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'benjamin',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'weather_etl_pipeline',
    default_args=default_args,
    description='ETL pipeline for weather data',
    schedule_interval='@daily',
    catchup=False,
) as dag:

    extract_task = PythonOperator(
        task_id='extract_weather_data',
        python_callable=extract_weather_data,
    )

    transform_task = PythonOperator(
        task_id='transform_weather_data',
        python_callable=transform_weather_data,
    )

    load_task = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
    )

    extract_task >> transform_task >> load_task
```

## ğŸ“Š RÃ©sultats

- **Volume traitÃ©** : ~10,000 records/jour
- **Temps d'exÃ©cution** : < 2 minutes
- **FiabilitÃ©** : 99.5% uptime sur 30 jours

## ğŸ”— Liens

- [**Code source sur GitHub**](https://github.com/ton-username/etl-weather-pipeline)
- [Documentation Airflow](https://airflow.apache.org/docs/)

---

*Projet rÃ©alisÃ© dans le cadre de ma formation Data Engineering.*
