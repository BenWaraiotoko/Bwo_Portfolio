---
title: "Pipeline ETL - DonnÃ©es mÃ©tÃ©o"
date: 2025-01-20
tags: ["python", "etl", "pandas", "postgresql", "api"]
categories: ["Projets"]
github: "https://github.com/ton-username/meteo-etl"
draft: false
---

## ğŸŒ¤ï¸ PrÃ©sentation du projet

Un pipeline ETL complet pour collecter, transformer et stocker des donnÃ©es mÃ©tÃ©orologiques. Ce projet m'a permis de mettre en pratique les concepts appris sur Codecademy.

## ğŸ¯ Objectifs

- Automatiser la collecte de donnÃ©es mÃ©tÃ©o depuis une API
- Nettoyer et transformer les donnÃ©es brutes
- Stocker dans une base PostgreSQL structurÃ©e
- GÃ©nÃ©rer des rapports quotidiens automatiquement

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenWeatherâ”‚  (Source)
â”‚     API     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Extract
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python    â”‚  (Transform)
â”‚   Script    â”‚  - Nettoyage
â”‚             â”‚  - Validation
â”‚             â”‚  - Enrichissement
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Load
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚  (Destination)
â”‚  Database   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Stack technique

- **Python 3.11** : Langage principal
- **Pandas** : Manipulation de donnÃ©es
- **psycopg2** : Connexion PostgreSQL
- **requests** : Appels API
- **Docker** : Conteneurisation de la base
- **schedule** : Automatisation des tÃ¢ches

## ğŸ’» Code exemple

### Extraction des donnÃ©es

```python
import requests
import pandas as pd
from datetime import datetime

def extract_weather_data(city: str, api_key: str) -> dict:
    """RÃ©cupÃ¨re les donnÃ©es mÃ©tÃ©o depuis l'API OpenWeather"""
    url = f"http://api.openweathermap.org/data/2.5/weather"
    params = {
        'q': city,
        'appid': api_key,
        'units': 'metric',
        'lang': 'fr'
    }
    
    response = requests.get(url, params=params)
    response.raise_for_status()
    
    return response.json()
```

### Transformation des donnÃ©es

```python
def transform_weather_data(raw_data: dict) -> pd.DataFrame:
    """Nettoie et structure les donnÃ©es"""
    transformed = {
        'timestamp': datetime.fromtimestamp(raw_data['dt']),
        'city': raw_data['name'],
        'country': raw_data['sys']['country'],
        'temperature': raw_data['main']['temp'],
        'feels_like': raw_data['main']['feels_like'],
        'humidity': raw_data['main']['humidity'],
        'pressure': raw_data['main']['pressure'],
        'weather': raw_data['weather'][0]['description'],
        'wind_speed': raw_data['wind']['speed']
    }
    
    df = pd.DataFrame([transformed])
    
    # Validation
    assert df['temperature'].between(-50, 60).all(), "TempÃ©rature invalide"
    assert df['humidity'].between(0, 100).all(), "HumiditÃ© invalide"
    
    return df
```

### Chargement dans PostgreSQL

```python
import psycopg2
from psycopg2.extras import execute_values

def load_to_postgres(df: pd.DataFrame, conn_params: dict):
    """Charge les donnÃ©es dans PostgreSQL"""
    conn = psycopg2.connect(**conn_params)
    cursor = conn.cursor()
    
    # CrÃ©ation de la table si elle n'existe pas
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS weather_data (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP NOT NULL,
            city VARCHAR(100),
            country VARCHAR(10),
            temperature NUMERIC(5,2),
            feels_like NUMERIC(5,2),
            humidity INTEGER,
            pressure INTEGER,
            weather VARCHAR(100),
            wind_speed NUMERIC(5,2),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Insertion des donnÃ©es
    columns = df.columns.tolist()
    values = [tuple(x) for x in df.to_numpy()]
    
    query = f"INSERT INTO weather_data ({','.join(columns)}) VALUES %s"
    execute_values(cursor, query, values)
    
    conn.commit()
    cursor.close()
    conn.close()
```

## ğŸ“Š RÃ©sultats

Le pipeline tourne depuis 2 semaines avec :
- âœ… **100% de disponibilitÃ©** (aucune erreur)
- âœ… **3 000+ entrÃ©es** collectÃ©es
- âœ… **ExÃ©cution toutes les heures** via cron
- âœ… **Logs complets** pour le monitoring

### Exemple de requÃªte analytique

```sql
-- TempÃ©rature moyenne par ville sur 7 jours
SELECT 
    city,
    DATE(timestamp) as date,
    ROUND(AVG(temperature), 1) as avg_temp,
    ROUND(AVG(humidity), 0) as avg_humidity
FROM weather_data
WHERE timestamp >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY city, DATE(timestamp)
ORDER BY city, date DESC;
```

## ğŸš€ AmÃ©liorations futures

- [ ] Ajouter plus de villes europÃ©ennes
- [ ] ImplÃ©menter un dashboard avec Plotly
- [ ] Migrer vers Airflow pour l'orchestration
- [ ] Ajouter des alertes (email) si anomalie dÃ©tectÃ©e
- [ ] DÃ©ployer sur AWS Lambda (serverless)

## ğŸ“š Apprentissages clÃ©s

Ce projet m'a permis de comprendre :

1. **Gestion des erreurs** : Retry logic, validation de donnÃ©es
2. **Optimisation SQL** : Index, requÃªtes agrÃ©gÃ©es performantes
3. **Docker** : Conteneurisation pour reproductibilitÃ©
4. **Logging** : Monitoring et debugging en production
5. **Architecture ETL** : SÃ©paration claire des responsabilitÃ©s

## ğŸ”— Concepts liÃ©s

{{< article-graph >}}

## ğŸ“¦ Code source

Le code complet est disponible sur GitHub : [meteo-etl](https://github.com/ton-username/meteo-etl)

```bash
# Installation
git clone https://github.com/ton-username/meteo-etl.git
cd meteo-etl

# Configuration
cp .env.example .env
# Ã‰dite .env avec ta clÃ© API OpenWeather

# Lancement avec Docker
docker-compose up -d

# ExÃ©cution manuelle
python etl_pipeline.py
```

---

*Ce projet fait partie de ma formation Codecademy Data Engineer - Module ETL Pipelines*